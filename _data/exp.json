{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A starter tutorial on __pytorch__. Aims at introducing pytorch through examples namely by implementing\n",
    "\n",
    "_1) Linear Regression_\n",
    "\n",
    "_2) Logistic Regression_\n",
    "\n",
    "_3) Simple Neural Network_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going into implementations let's understand the basic unit of pytorch i.e a __Tensor__.\n",
    "\n",
    "A tensor is a simple n dimensional array similar to numpy but can run on GPU's. Hence computation can be spedup.\n",
    "\n",
    "Another important feature that pytorch provides is __autograd__. i.e automatic differentiation wrt variables, this helps us obtain the gradient during the backpropagation step in any neural network\n",
    "\n",
    "Now that we know the basic inners of pytorch. Let's try and implement the most basic ML algo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# tried but got nothing to say about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#declaring data\n",
    "x = torch.Tensor([[1.0],[2.0],[4.0],[5.0]])\n",
    "y = torch.Tensor([[2.0],[4.0],[8.0],[10.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __autograd__ feature we mentioned previously cannot be directly used with pytorch tensors. We need to wrap these tensors in __Variables__ to use the feature.\n",
    "\n",
    "In the computational graph we are going to build, this __variable__ object is going to be one of the nodes.\n",
    "\n",
    "If __w__ is a Variable then __w.data__ is a Tensor, and __w.grad__ is another Variable holding the gradient of x with respect to some scalar value (usually the loss function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "# wrapping up the data in variables\n",
    "x = Variable(x)\n",
    "y = Variable(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.autograd.variable.Variable'>\n",
      "<class 'torch.FloatTensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x))       # Variable\n",
    "print(type(x.data))  # Tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1\n",
      " 2\n",
      " 4\n",
      " 5\n",
      "[torch.FloatTensor of size 4x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# variable with values\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = Variable(torch.Tensor([[1.0], [2.0], [3.0], [4.0]]))\n",
    "y_data = Variable(torch.Tensor([[0.0], [0.0], [1.0], [1.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.module):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x_data = Variable(torch.Tensor([[1.0], [2.0], [3.0]]))\n",
    "y_data = Variable(torch.Tensor([[2.0], [4.0], [6.0]]))\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 80.16229248046875\n",
      "1 35.98162841796875\n",
      "2 16.309410095214844\n",
      "3 7.547708511352539\n",
      "4 3.643120765686035\n",
      "5 1.900839924812317\n",
      "6 1.121216058731079\n",
      "7 0.7701972126960754\n",
      "8 0.6100374460220337\n",
      "9 0.5348997712135315\n",
      "10 0.4976658821105957\n",
      "11 0.47736072540283203\n",
      "12 0.4646448493003845\n",
      "13 0.4553607702255249\n",
      "14 0.4476558566093445\n",
      "15 0.44070565700531006\n",
      "16 0.4341421127319336\n",
      "17 0.4278004765510559\n",
      "18 0.4216066300868988\n",
      "19 0.41552746295928955\n",
      "20 0.4095466136932373\n",
      "21 0.40365681052207947\n",
      "22 0.39785367250442505\n",
      "23 0.3921350836753845\n",
      "24 0.3864992558956146\n",
      "25 0.38094425201416016\n",
      "26 0.3754695653915405\n",
      "27 0.37007319927215576\n",
      "28 0.3647550046443939\n",
      "29 0.3595128655433655\n",
      "30 0.3543459475040436\n",
      "31 0.34925368428230286\n",
      "32 0.34423398971557617\n",
      "33 0.3392869234085083\n",
      "34 0.334410697221756\n",
      "35 0.3296048045158386\n",
      "36 0.3248680830001831\n",
      "37 0.32019925117492676\n",
      "38 0.31559714674949646\n",
      "39 0.3110617697238922\n",
      "40 0.3065911829471588\n",
      "41 0.30218520760536194\n",
      "42 0.29784220457077026\n",
      "43 0.2935618460178375\n",
      "44 0.2893427014350891\n",
      "45 0.28518450260162354\n",
      "46 0.2810860574245453\n",
      "47 0.2770463526248932\n",
      "48 0.27306443452835083\n",
      "49 0.26914024353027344\n",
      "50 0.26527243852615356\n",
      "51 0.26145994663238525\n",
      "52 0.25770217180252075\n",
      "53 0.2539987564086914\n",
      "54 0.2503484785556793\n",
      "55 0.24675066769123077\n",
      "56 0.24320416152477264\n",
      "57 0.23970893025398254\n",
      "58 0.23626409471035004\n",
      "59 0.2328687310218811\n",
      "60 0.22952190041542053\n",
      "61 0.2262234091758728\n",
      "62 0.2229723036289215\n",
      "63 0.21976782381534576\n",
      "64 0.21660944819450378\n",
      "65 0.2134961634874344\n",
      "66 0.21042820811271667\n",
      "67 0.20740389823913574\n",
      "68 0.20442327857017517\n",
      "69 0.20148535072803497\n",
      "70 0.19858942925930023\n",
      "71 0.19573543965816498\n",
      "72 0.19292238354682922\n",
      "73 0.19014984369277954\n",
      "74 0.18741723895072937\n",
      "75 0.18472373485565186\n",
      "76 0.18206886947155\n",
      "77 0.17945221066474915\n",
      "78 0.1768730729818344\n",
      "79 0.17433114349842072\n",
      "80 0.17182573676109314\n",
      "81 0.16935643553733826\n",
      "82 0.16692249476909637\n",
      "83 0.1645236760377884\n",
      "84 0.1621590554714203\n",
      "85 0.15982863306999207\n",
      "86 0.15753157436847687\n",
      "87 0.15526768565177917\n",
      "88 0.15303614735603333\n",
      "89 0.15083694458007812\n",
      "90 0.14866912364959717\n",
      "91 0.14653247594833374\n",
      "92 0.14442658424377441\n",
      "93 0.14235088229179382\n",
      "94 0.14030522108078003\n",
      "95 0.1382887065410614\n",
      "96 0.13630136847496033\n",
      "97 0.13434243202209473\n",
      "98 0.1324116438627243\n",
      "99 0.13050872087478638\n",
      "100 0.12863317131996155\n",
      "101 0.12678448855876923\n",
      "102 0.12496243417263031\n",
      "103 0.12316662073135376\n",
      "104 0.12139637023210526\n",
      "105 0.11965176463127136\n",
      "106 0.11793212592601776\n",
      "107 0.11623741686344147\n",
      "108 0.11456668376922607\n",
      "109 0.1129203513264656\n",
      "110 0.11129751801490784\n",
      "111 0.10969781875610352\n",
      "112 0.1081213727593422\n",
      "113 0.10656757652759552\n",
      "114 0.10503582656383514\n",
      "115 0.10352639108896255\n",
      "116 0.1020386666059494\n",
      "117 0.10057207942008972\n",
      "118 0.0991266667842865\n",
      "119 0.09770210832357407\n",
      "120 0.09629799425601959\n",
      "121 0.09491396695375443\n",
      "122 0.09354989230632782\n",
      "123 0.09220550954341888\n",
      "124 0.09088034927845001\n",
      "125 0.08957432210445404\n",
      "126 0.0882868692278862\n",
      "127 0.08701813220977783\n",
      "128 0.08576761931180954\n",
      "129 0.08453483879566193\n",
      "130 0.08332005143165588\n",
      "131 0.08212260901927948\n",
      "132 0.08094238489866257\n",
      "133 0.07977914810180664\n",
      "134 0.07863248884677887\n",
      "135 0.07750241458415985\n",
      "136 0.07638862729072571\n",
      "137 0.07529078423976898\n",
      "138 0.07420884072780609\n",
      "139 0.07314233481884003\n",
      "140 0.0720909982919693\n",
      "141 0.07105494290590286\n",
      "142 0.07003387808799744\n",
      "143 0.06902734935283661\n",
      "144 0.06803539395332336\n",
      "145 0.06705757230520248\n",
      "146 0.0660937950015068\n",
      "147 0.06514393538236618\n",
      "148 0.06420768797397614\n",
      "149 0.06328500062227249\n",
      "150 0.062375493347644806\n",
      "151 0.06147913262248039\n",
      "152 0.060595571994781494\n",
      "153 0.05972457304596901\n",
      "154 0.058866314589977264\n",
      "155 0.05802030488848686\n",
      "156 0.05718644708395004\n",
      "157 0.056364547461271286\n",
      "158 0.055554620921611786\n",
      "159 0.05475613847374916\n",
      "160 0.053969208151102066\n",
      "161 0.05319365859031677\n",
      "162 0.052429117262363434\n",
      "163 0.05167560651898384\n",
      "164 0.050932906568050385\n",
      "165 0.050200968980789185\n",
      "166 0.04947947710752487\n",
      "167 0.04876840114593506\n",
      "168 0.04806751757860184\n",
      "169 0.04737669229507446\n",
      "170 0.0466957651078701\n",
      "171 0.04602479934692383\n",
      "172 0.04536323994398117\n",
      "173 0.04471142962574959\n",
      "174 0.04406879097223282\n",
      "175 0.04343550652265549\n",
      "176 0.042811229825019836\n",
      "177 0.04219599813222885\n",
      "178 0.041589587926864624\n",
      "179 0.04099177569150925\n",
      "180 0.04040266200900078\n",
      "181 0.039822060614824295\n",
      "182 0.03924966976046562\n",
      "183 0.03868567943572998\n",
      "184 0.038129717111587524\n",
      "185 0.03758171573281288\n",
      "186 0.03704165294766426\n",
      "187 0.03650931268930435\n",
      "188 0.035984501242637634\n",
      "189 0.03546743839979172\n",
      "190 0.03495771437883377\n",
      "191 0.03445529192686081\n",
      "192 0.03396008908748627\n",
      "193 0.03347200155258179\n",
      "194 0.03299105912446976\n",
      "195 0.03251689672470093\n",
      "196 0.032049573957920074\n",
      "197 0.031588971614837646\n",
      "198 0.03113497421145439\n",
      "199 0.030687469989061356\n",
      "200 0.030246544629335403\n",
      "201 0.02981180138885975\n",
      "202 0.029383312910795212\n",
      "203 0.02896108478307724\n",
      "204 0.028544869273900986\n",
      "205 0.028134610503911972\n",
      "206 0.027730276808142662\n",
      "207 0.027331825345754623\n",
      "208 0.026938963681459427\n",
      "209 0.026551764458417892\n",
      "210 0.02617025002837181\n",
      "211 0.02579409070312977\n",
      "212 0.02542341873049736\n",
      "213 0.025058038532733917\n",
      "214 0.02469789609313011\n",
      "215 0.024342959746718407\n",
      "216 0.02399311400949955\n",
      "217 0.023648234084248543\n",
      "218 0.02330843359231949\n",
      "219 0.02297346293926239\n",
      "220 0.022643333300948143\n",
      "221 0.022317856550216675\n",
      "222 0.021997125819325447\n",
      "223 0.02168102189898491\n",
      "224 0.02136942930519581\n",
      "225 0.021062295883893967\n",
      "226 0.020759588107466698\n",
      "227 0.020461300387978554\n",
      "228 0.020167186856269836\n",
      "229 0.019877318292856216\n",
      "230 0.019591718912124634\n",
      "231 0.01931011863052845\n",
      "232 0.01903260126709938\n",
      "233 0.018759116530418396\n",
      "234 0.01848948560655117\n",
      "235 0.018223751336336136\n",
      "236 0.017961811274290085\n",
      "237 0.017703678458929062\n",
      "238 0.017449233680963516\n",
      "239 0.017198484390974045\n",
      "240 0.01695133000612259\n",
      "241 0.016707705333828926\n",
      "242 0.016467643901705742\n",
      "243 0.016230985522270203\n",
      "244 0.01599765755236149\n",
      "245 0.015767794102430344\n",
      "246 0.015541152097284794\n",
      "247 0.015317755751311779\n",
      "248 0.015097662806510925\n",
      "249 0.014880722388625145\n",
      "250 0.014666831120848656\n",
      "251 0.014456018805503845\n",
      "252 0.014248315244913101\n",
      "253 0.014043513685464859\n",
      "254 0.013841693289577961\n",
      "255 0.013642738573253155\n",
      "256 0.013446686789393425\n",
      "257 0.013253460638225079\n",
      "258 0.013063020072877407\n",
      "259 0.012875232845544815\n",
      "260 0.01269023958593607\n",
      "261 0.012507839128375053\n",
      "262 0.012328063137829304\n",
      "263 0.012150921858847141\n",
      "264 0.011976265348494053\n",
      "265 0.01180417463183403\n",
      "266 0.01163446344435215\n",
      "267 0.011467303149402142\n",
      "268 0.011302487924695015\n",
      "269 0.011140051297843456\n",
      "270 0.01097994763404131\n",
      "271 0.010822128504514694\n",
      "272 0.010666603222489357\n",
      "273 0.010513284243643284\n",
      "274 0.010362228378653526\n",
      "275 0.010213317349553108\n",
      "276 0.010066527873277664\n",
      "277 0.009921885095536709\n",
      "278 0.009779231622815132\n",
      "279 0.009638706222176552\n",
      "280 0.009500185027718544\n",
      "281 0.009363679215312004\n",
      "282 0.009229077026247978\n",
      "283 0.009096447378396988\n",
      "284 0.00896572507917881\n",
      "285 0.008836833760142326\n",
      "286 0.008709868416190147\n",
      "287 0.008584690280258656\n",
      "288 0.008461300283670425\n",
      "289 0.008339690044522285\n",
      "290 0.008219845592975616\n",
      "291 0.008101742714643478\n",
      "292 0.007985309697687626\n",
      "293 0.007870519533753395\n",
      "294 0.007757420651614666\n",
      "295 0.007645928300917149\n",
      "296 0.007536060176789761\n",
      "297 0.007427714299410582\n",
      "298 0.007320963777601719\n",
      "299 0.0072157662361860275\n",
      "300 0.007112085819244385\n",
      "301 0.007009865250438452\n",
      "302 0.006909085903316736\n",
      "303 0.006809842772781849\n",
      "304 0.006711960304528475\n",
      "305 0.006615509279072285\n",
      "306 0.006520405411720276\n",
      "307 0.006426688749343157\n",
      "308 0.006334358360618353\n",
      "309 0.0062432982958853245\n",
      "310 0.00615360401570797\n",
      "311 0.006065129302442074\n",
      "312 0.005977969616651535\n",
      "313 0.005892079323530197\n",
      "314 0.005807382520288229\n",
      "315 0.005723900161683559\n",
      "316 0.005641665309667587\n",
      "317 0.005560554563999176\n",
      "318 0.0054806931875646114\n",
      "319 0.005401914473623037\n",
      "320 0.005324273835867643\n",
      "321 0.005247751250863075\n",
      "322 0.005172335542738438\n",
      "323 0.005097983404994011\n",
      "324 0.005024755373597145\n",
      "325 0.004952507093548775\n",
      "326 0.0048813470639288425\n",
      "327 0.004811197519302368\n",
      "328 0.004742044489830732\n",
      "329 0.00467388890683651\n",
      "330 0.004606695845723152\n",
      "331 0.004540495574474335\n",
      "332 0.004475271794945002\n",
      "333 0.00441094022244215\n",
      "334 0.004347569774836302\n",
      "335 0.004285091534256935\n",
      "336 0.004223473370075226\n",
      "337 0.004162815399467945\n",
      "338 0.004102954175323248\n",
      "339 0.00404400285333395\n",
      "340 0.003985886462032795\n",
      "341 0.003928623627871275\n",
      "342 0.0038721514865756035\n",
      "343 0.0038164870347827673\n",
      "344 0.003761635860428214\n",
      "345 0.0037075744476169348\n",
      "346 0.003654294414445758\n",
      "347 0.003601801348850131\n",
      "348 0.0035500209778547287\n",
      "349 0.003499000333249569\n",
      "350 0.0034487280063331127\n",
      "351 0.0033991483505815268\n",
      "352 0.0033502778969705105\n",
      "353 0.0033021452836692333\n",
      "354 0.003254679497331381\n",
      "355 0.003207916161045432\n",
      "356 0.0031618019565939903\n",
      "357 0.0031163753010332584\n",
      "358 0.003071586834266782\n",
      "359 0.0030274454038590193\n",
      "360 0.002983919344842434\n",
      "361 0.002941048238426447\n",
      "362 0.002898779232054949\n",
      "363 0.002857126295566559\n",
      "364 0.0028160777874290943\n",
      "365 0.0027755957562476397\n",
      "366 0.002735704416409135\n",
      "367 0.0026963709387928247\n",
      "368 0.0026576428208500147\n",
      "369 0.0026194569654762745\n",
      "370 0.0025817863643169403\n",
      "371 0.002544696908444166\n",
      "372 0.0025081378407776356\n",
      "373 0.002472077263519168\n",
      "374 0.0024365591816604137\n",
      "375 0.0024015174712985754\n",
      "376 0.0023670182563364506\n",
      "377 0.002332979114726186\n",
      "378 0.0022994792088866234\n",
      "379 0.002266407012939453\n",
      "380 0.00223386287689209\n",
      "381 0.002201745519414544\n",
      "382 0.002170087769627571\n",
      "383 0.002138909650966525\n",
      "384 0.0021081760060042143\n",
      "385 0.0020778635516762733\n",
      "386 0.0020480072125792503\n",
      "387 0.0020185760222375393\n",
      "388 0.0019895576406270266\n",
      "389 0.001960975816473365\n",
      "390 0.0019327925983816385\n",
      "391 0.0019050086848437786\n",
      "392 0.001877647708170116\n",
      "393 0.001850672299042344\n",
      "394 0.0018240706995129585\n",
      "395 0.0017978344112634659\n",
      "396 0.0017720109317451715\n",
      "397 0.0017465490382164717\n",
      "398 0.001721435459330678\n",
      "399 0.0016967002302408218\n",
      "400 0.0016723056323826313\n",
      "401 0.0016482836799696088\n",
      "402 0.001624604337848723\n",
      "403 0.001601261319592595\n",
      "404 0.0015782375121489167\n",
      "405 0.001555552356876433\n",
      "406 0.0015331952599808574\n",
      "407 0.0015111628454178572\n",
      "408 0.0014894335763528943\n",
      "409 0.001468046335503459\n",
      "410 0.0014469311572611332\n",
      "411 0.001426144503057003\n",
      "412 0.001405663089826703\n",
      "413 0.001385438023135066\n",
      "414 0.0013655379880219698\n",
      "415 0.0013459203764796257\n",
      "416 0.0013265631860122085\n",
      "417 0.0013074935413897038\n",
      "418 0.0012887247139587998\n",
      "419 0.0012701849918812513\n",
      "420 0.0012519522570073605\n",
      "421 0.001233947346918285\n",
      "422 0.0012162167113274336\n",
      "423 0.0011987373000010848\n",
      "424 0.0011815084144473076\n",
      "425 0.0011645311024039984\n",
      "426 0.0011477909283712506\n",
      "427 0.0011313088471069932\n",
      "428 0.0011150294449180365\n",
      "429 0.0010990242008119822\n",
      "430 0.0010832208208739758\n",
      "431 0.0010676628444343805\n",
      "432 0.00105231162160635\n",
      "433 0.0010371766984462738\n",
      "434 0.0010222683195024729\n",
      "435 0.0010075990576297045\n",
      "436 0.0009931144304573536\n",
      "437 0.0009788402821868658\n",
      "438 0.0009647652041167021\n",
      "439 0.0009509014198556542\n",
      "440 0.0009372466010972857\n",
      "441 0.0009237664053216577\n",
      "442 0.0009104908676818013\n",
      "443 0.0008974125375971198\n",
      "444 0.0008845083648338914\n",
      "445 0.0008717886521480978\n",
      "446 0.0008592734229750931\n",
      "447 0.0008469265885651112\n",
      "448 0.0008347464026883245\n",
      "449 0.0008227512007579207\n",
      "450 0.0008109271293506026\n",
      "451 0.0007992758182808757\n",
      "452 0.0007877969765104353\n",
      "453 0.0007764708716422319\n",
      "454 0.0007653007050976157\n",
      "455 0.0007543168612755835\n",
      "456 0.0007434614817611873\n",
      "457 0.0007327888160943985\n",
      "458 0.0007222525891847908\n",
      "459 0.0007118717767298222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460 0.0007016416639089584\n",
      "461 0.000691551249474287\n",
      "462 0.0006816114182583988\n",
      "463 0.0006718203076161444\n",
      "464 0.0006621584761887789\n",
      "465 0.0006526571814902127\n",
      "466 0.0006432775990106165\n",
      "467 0.0006340332911349833\n",
      "468 0.0006249184370972216\n",
      "469 0.0006159257609397173\n",
      "470 0.0006070713861845434\n",
      "471 0.000598357932176441\n",
      "472 0.0005897582741454244\n",
      "473 0.0005812853341922164\n",
      "474 0.0005729238037019968\n",
      "475 0.0005646933568641543\n",
      "476 0.0005565800238400698\n",
      "477 0.0005485799047164619\n",
      "478 0.0005406903801485896\n",
      "479 0.0005329305422492325\n",
      "480 0.0005252747214399278\n",
      "481 0.000517725944519043\n",
      "482 0.0005102790310047567\n",
      "483 0.0005029525491409004\n",
      "484 0.0004957191995345056\n",
      "485 0.0004885948728770018\n",
      "486 0.00048157176934182644\n",
      "487 0.0004746451450046152\n",
      "488 0.00046783662401139736\n",
      "489 0.00046111346455290914\n",
      "490 0.00045447368756867945\n",
      "491 0.0004479485214687884\n",
      "492 0.00044150964822620153\n",
      "493 0.0004351658863015473\n",
      "494 0.0004289161879569292\n",
      "495 0.00042274879524484277\n",
      "496 0.00041667826008051634\n",
      "497 0.0004106846172362566\n",
      "498 0.00040478119626641273\n",
      "499 0.00039897041278891265\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# our model\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(500):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss.data[0])\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 7.9770\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(hour_var).data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (after training) 4 7.977038860321045\n"
     ]
    }
   ],
   "source": [
    "# After training\n",
    "hour_var = Variable(torch.Tensor([[4.0]]))\n",
    "print(\"predict (after training)\",  4, model.forward(hour_var).data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4],\n",
       "       [5, 6]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2], [3, 4], [5, 6]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4]\n"
     ]
    }
   ],
   "source": [
    "print a[[0,1], [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([5, 6, 7, 8]), (4,))\n",
      "(array([[ 5,  6,  7,  8],\n",
      "       [ 9, 10, 11, 12]]), (2, 4))\n"
     ]
    }
   ],
   "source": [
    "row_r1 = a[1, :]    # Rank 1 view of the second row of a\n",
    "row_r2 = a[1:3, :]  # Rank 2 view of the second row of a\n",
    "print(row_r1, row_r1.shape)  # Prints \"[5 6 7 8] (4,)\"\n",
    "print(row_r2, row_r2.shape)  # Prints \"[[5 6 7 8]] (1, 4)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 2,  6, 10]), (3,))\n",
      "(array([[ 2],\n",
      "       [ 6],\n",
      "       [10]]), (3, 1))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We can make the same distinction when accessing columns of an array:\n",
    "col_r1 = a[:, 1]\n",
    "col_r2 = a[:, 1:2]\n",
    "print(col_r1, col_r1.shape)  # Prints \"[ 2  6 10] (3,)\"\n",
    "print(col_r2, col_r2.shape)  # Prints \"[[ 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = [1,0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [ 4,  5,  6],\n",
       "       [ 7,  8,  9],\n",
       "       [10, 11, 12]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a[np.arange(4),b] += 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 14, 18, 22])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[np.arange(4),b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bool_idx = (a>10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True, False],\n",
       "       [ True, False, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True,  True]], dtype=bool)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 14, 18, 11, 22])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[bool_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12 14 18 11 22]\n"
     ]
    }
   ],
   "source": [
    "print a[a>10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([[1,2],[3,4]])\n",
    "y = np.array([[5,6],[7,8]])\n",
    "\n",
    "v = np.array([9,10])\n",
    "w = np.array([11, 12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(v, w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29 67]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(x, v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19 22]\n",
      " [43 50]]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(x, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[4 6]\n",
      "[3 7]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[1,2],[3,4]])\n",
    "\n",
    "print(np.sum(x))  # Compute sum of all elements; prints \"10\"\n",
    "print(np.sum(x, axis=0))  # Compute sum of each column; prints \"[4 6]\"\n",
    "print(np.sum(x, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 6])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(x,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([[1,2],[1,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [2 3]]\n"
     ]
    }
   ],
   "source": [
    "print x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([[1,2,3], [4,5,6]])\n",
    "v = np.array([1,2,3])  # v has shape (3,)\n",
    "w = np.array([4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 4, 6],\n",
       "       [5, 7, 9]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x+v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 4],\n",
       "       [2, 5],\n",
       "       [3, 6]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  9],\n",
       "       [ 6, 10],\n",
       "       [ 7, 11]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.T+w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5]\n",
      "[[4]\n",
      " [5]]\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "print w\n",
    "print np.reshape(w,(2,1))\n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  6  7]\n",
      " [ 9 10 11]]\n"
     ]
    }
   ],
   "source": [
    "print(x + np.reshape(w, (2, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
